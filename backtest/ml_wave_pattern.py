#!/usr/bin/env python3
"""
ML íŒŒë™ íŒ¨í„´ ë¶„ì„ - ë³¼ë¥¨/ë¸íƒ€ ì¶”ì„¸ í•™ìŠµ

í•µì‹¬ ì•„ì´ë””ì–´:
1. ë ˆì¸ì§€ ë‚´ í‰ê·  ë³¼ë¥¨/ë¸íƒ€ (ê¸°ì¤€ì„ )
2. í„°ì¹˜ë¡œ ì˜¤ëŠ” íŒŒë™ì˜ ë³¼ë¥¨/ë¸íƒ€ ì¶”ì„¸ (ì¦ê°€? ê°ì†Œ?)
3. í„°ì¹˜ ìº”ë“¤ì—ì„œì˜ ë°˜ì‘

í•™ìŠµ: 2022-2023
í…ŒìŠ¤íŠ¸: 2024-2025
"""

import os
import sys
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

sys.path.insert(0, os.path.dirname(__file__))
from parse_data import load_candles
from ml_channel_proper_mtf import build_htf_channels


def extract_wave_features(ltf_candles, idx, channel, direction):
    """íŒŒë™ íŒ¨í„´ í”¼ì²˜ ì¶”ì¶œ."""

    closes = ltf_candles['close'].values
    highs = ltf_candles['high'].values
    lows = ltf_candles['low'].values
    volumes = ltf_candles['volume'].values
    deltas = ltf_candles['delta'].values if 'delta' in ltf_candles.columns else np.zeros(len(ltf_candles))

    # === 1. ë ˆì¸ì§€ ë‚´ í‰ê·  (ê¸°ì¤€ì„ ) ===
    range_lookback = 20
    start = max(0, idx - range_lookback)

    range_volumes = volumes[start:idx]
    range_deltas = deltas[start:idx]

    range_avg_vol = np.mean(range_volumes) if len(range_volumes) > 0 else 1
    range_avg_delta = np.mean(range_deltas) if len(range_deltas) > 0 else 0
    range_avg_abs_delta = np.mean(np.abs(range_deltas)) if len(range_deltas) > 0 else 1
    range_vol_std = np.std(range_volumes) if len(range_volumes) > 1 else 1

    # === 2. íŒŒë™ ì¶”ì„¸ (í„°ì¹˜ ì§ì „ 5ê°œ ìº”ë“¤) ===
    wave_lookback = 5
    wave_start = max(0, idx - wave_lookback)

    wave_volumes = volumes[wave_start:idx+1]
    wave_deltas = deltas[wave_start:idx+1]

    # ë³¼ë¥¨ ì¶”ì„¸ (ê¸°ìš¸ê¸°)
    if len(wave_volumes) >= 2:
        vol_slope = np.polyfit(range(len(wave_volumes)), wave_volumes, 1)[0]
        vol_trend = vol_slope / (range_avg_vol + 1e-10)  # ì •ê·œí™”
    else:
        vol_trend = 0

    # ë¸íƒ€ ì¶”ì„¸ (ê¸°ìš¸ê¸°)
    if len(wave_deltas) >= 2:
        delta_slope = np.polyfit(range(len(wave_deltas)), wave_deltas, 1)[0]
        delta_trend = delta_slope / (range_avg_abs_delta + 1e-10)
    else:
        delta_trend = 0

    # ë³¼ë¥¨/ë¸íƒ€ ê°€ì†ë„ (ì¶”ì„¸ì˜ ì¶”ì„¸)
    if len(wave_volumes) >= 3:
        vol_diff = np.diff(wave_volumes)
        vol_accel = np.mean(np.diff(vol_diff)) if len(vol_diff) >= 2 else 0
    else:
        vol_accel = 0

    if len(wave_deltas) >= 3:
        delta_diff = np.diff(wave_deltas)
        delta_accel = np.mean(np.diff(delta_diff)) if len(delta_diff) >= 2 else 0
    else:
        delta_accel = 0

    # === 3. í„°ì¹˜ ìº”ë“¤ ë°˜ì‘ ===
    touch_vol = volumes[idx]
    touch_delta = deltas[idx]

    # ë ˆì¸ì§€ ëŒ€ë¹„
    vol_vs_range = touch_vol / (range_avg_vol + 1e-10)
    delta_vs_range = touch_delta / (range_avg_abs_delta + 1e-10)
    vol_zscore = (touch_vol - range_avg_vol) / (range_vol_std + 1e-10)

    # íŒŒë™ í‰ê·  ëŒ€ë¹„
    wave_avg_vol = np.mean(wave_volumes[:-1]) if len(wave_volumes) > 1 else touch_vol
    wave_avg_delta = np.mean(wave_deltas[:-1]) if len(wave_deltas) > 1 else touch_delta

    vol_vs_wave = touch_vol / (wave_avg_vol + 1e-10)
    delta_vs_wave = touch_delta / (np.abs(wave_avg_delta) + 1e-10) if wave_avg_delta != 0 else 0

    # === 4. ë°©í–¥ ì •ë ¬ ===
    # LONG: ë¸íƒ€ ì–‘ìˆ˜ê°€ ì¢‹ìŒ, SHORT: ë¸íƒ€ ìŒìˆ˜ê°€ ì¢‹ìŒ
    if direction == 'LONG':
        delta_aligned = 1 if touch_delta > 0 else 0
        delta_trend_aligned = 1 if delta_trend > 0 else 0  # ë¸íƒ€ ìƒìŠ¹ ì¶”ì„¸
    else:
        delta_aligned = 1 if touch_delta < 0 else 0
        delta_trend_aligned = 1 if delta_trend < 0 else 0  # ë¸íƒ€ í•˜ë½ ì¶”ì„¸

    # === 5. ì¶”ê°€ í”¼ì²˜ ===
    # ë³¼ë¥¨ ê¸‰ì¦/ê¸‰ê°
    vol_spike = 1 if vol_vs_range >= 2.0 else 0
    vol_low = 1 if vol_vs_range <= 0.5 else 0

    # ë¸íƒ€ ë°˜ì „ (íŒŒë™ê³¼ ë°˜ëŒ€ ë°©í–¥)
    delta_reversal = 1 if (wave_avg_delta < 0 and touch_delta > 0) or (wave_avg_delta > 0 and touch_delta < 0) else 0

    # CVD (ëˆ„ì  ë¸íƒ€) ì¶”ì„¸
    cvd_wave = np.sum(wave_deltas)
    cvd_range = np.sum(range_deltas)

    # ìº”ë“¤ íŒ¨í„´
    open_price = ltf_candles['open'].values[idx]
    close_price = closes[idx]
    candle_body = close_price - open_price
    candle_range = highs[idx] - lows[idx]
    body_ratio = abs(candle_body) / (candle_range + 1e-10)

    is_bullish = 1 if close_price > open_price else 0

    # ê¼¬ë¦¬ ë¹„ìœ¨
    if direction == 'LONG':
        wick_ratio = (min(open_price, close_price) - lows[idx]) / (candle_range + 1e-10)  # í•˜ë‹¨ ê¼¬ë¦¬
    else:
        wick_ratio = (highs[idx] - max(open_price, close_price)) / (candle_range + 1e-10)  # ìƒë‹¨ ê¼¬ë¦¬

    features = {
        # ë ˆì¸ì§€ ëŒ€ë¹„
        'vol_vs_range': vol_vs_range,
        'delta_vs_range': delta_vs_range,
        'vol_zscore': vol_zscore,

        # íŒŒë™ ì¶”ì„¸
        'vol_trend': vol_trend,
        'delta_trend': delta_trend,
        'vol_accel': vol_accel / (range_avg_vol + 1e-10),
        'delta_accel': delta_accel / (range_avg_abs_delta + 1e-10),

        # íŒŒë™ ëŒ€ë¹„
        'vol_vs_wave': vol_vs_wave,
        'delta_vs_wave': delta_vs_wave,

        # ë°©í–¥ ì •ë ¬
        'delta_aligned': delta_aligned,
        'delta_trend_aligned': delta_trend_aligned,

        # ë³¼ë¥¨/ë¸íƒ€ ìƒíƒœ
        'vol_spike': vol_spike,
        'vol_low': vol_low,
        'delta_reversal': delta_reversal,

        # CVD
        'cvd_wave': cvd_wave / (range_avg_abs_delta * wave_lookback + 1e-10),
        'cvd_range': cvd_range / (range_avg_abs_delta * range_lookback + 1e-10),

        # ìº”ë“¤ íŒ¨í„´
        'body_ratio': body_ratio,
        'is_bullish': is_bullish,
        'wick_ratio': wick_ratio,

        # ë°©í–¥
        'is_long': 1 if direction == 'LONG' else 0,
    }

    return features


def simulate_trade(highs, lows, idx, direction, entry, sl, tp1):
    """TP1 ë„ë‹¬ ì—¬ë¶€ë¡œ ì„±ê³µ íŒì •."""
    for j in range(idx + 1, min(idx + 150, len(highs))):
        if direction == 'LONG':
            if lows[j] <= sl:
                return 0  # ì‹¤íŒ¨
            if highs[j] >= tp1:
                return 1  # ì„±ê³µ
        else:
            if highs[j] >= sl:
                return 0
            if lows[j] <= tp1:
                return 1
    return 0  # íƒ€ì„ì•„ì›ƒ = ì‹¤íŒ¨


def collect_data(htf_candles, ltf_candles, channels_dict, tf_ratio=4):
    """í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘."""
    data = []
    traded_keys = set()

    ltf_highs = ltf_candles['high'].values
    ltf_lows = ltf_candles['low'].values
    ltf_closes = ltf_candles['close'].values

    sl_buffer = 0.0008
    touch_threshold = 0.003

    for i in range(100, len(ltf_candles) - 200):
        htf_idx = i // tf_ratio
        channel = channels_dict.get(htf_idx - 1)

        if not channel:
            continue

        close = ltf_closes[i]
        high = ltf_highs[i]
        low = ltf_lows[i]
        mid = (channel.resistance + channel.support) / 2

        bounce_key = (round(channel.support), round(channel.resistance), i // 20)
        if bounce_key in traded_keys:
            continue

        # Support touch â†’ LONG
        if low <= channel.support * (1 + touch_threshold) and close > channel.support:
            entry = close
            sl = channel.support * (1 - sl_buffer)
            tp1 = mid

            if entry > sl and tp1 > entry:
                features = extract_wave_features(ltf_candles, i, channel, 'LONG')
                success = simulate_trade(ltf_highs, ltf_lows, i, 'LONG', entry, sl, tp1)

                features['success'] = success
                features['direction'] = 'LONG'
                features['idx'] = i
                features['entry'] = entry
                features['sl'] = sl
                features['tp1'] = tp1

                data.append(features)
                traded_keys.add(bounce_key)

        # Resistance touch â†’ SHORT
        elif high >= channel.resistance * (1 - touch_threshold) and close < channel.resistance:
            entry = close
            sl = channel.resistance * (1 + sl_buffer)
            tp1 = mid

            if sl > entry and entry > tp1:
                features = extract_wave_features(ltf_candles, i, channel, 'SHORT')
                success = simulate_trade(ltf_highs, ltf_lows, i, 'SHORT', entry, sl, tp1)

                features['success'] = success
                features['direction'] = 'SHORT'
                features['idx'] = i
                features['entry'] = entry
                features['sl'] = sl
                features['tp1'] = tp1

                data.append(features)
                traded_keys.add(bounce_key)

    return pd.DataFrame(data)


def train_and_evaluate(train_df, test_df):
    """ML ëª¨ë¸ í•™ìŠµ ë° í‰ê°€."""

    feature_cols = [
        'vol_vs_range', 'delta_vs_range', 'vol_zscore',
        'vol_trend', 'delta_trend', 'vol_accel', 'delta_accel',
        'vol_vs_wave', 'delta_vs_wave',
        'delta_aligned', 'delta_trend_aligned',
        'vol_spike', 'vol_low', 'delta_reversal',
        'cvd_wave', 'cvd_range',
        'body_ratio', 'is_bullish', 'wick_ratio',
        'is_long',
    ]

    X_train = train_df[feature_cols].values
    y_train = train_df['success'].values
    X_test = test_df[feature_cols].values
    y_test = test_df['success'].values

    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ëª¨ë¸ í•™ìŠµ
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=5,
        min_samples_leaf=10,
        random_state=42,
        class_weight='balanced'
    )
    model.fit(X_train_scaled, y_train)

    # ì˜ˆì¸¡
    train_pred = model.predict(X_train_scaled)
    test_pred = model.predict(X_test_scaled)
    train_proba = model.predict_proba(X_train_scaled)[:, 1]
    test_proba = model.predict_proba(X_test_scaled)[:, 1]

    # Feature importance
    importance = dict(zip(feature_cols, model.feature_importances_))
    importance = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))

    return model, scaler, train_pred, test_pred, train_proba, test_proba, importance


def backtest_with_filter(df, proba, threshold, label):
    """í•„í„° ì ìš© ë°±í…ŒìŠ¤íŠ¸."""
    mask = proba >= threshold
    filtered = df[mask]

    if len(filtered) == 0:
        return None

    total = len(filtered)
    wins = filtered['success'].sum()
    wr = wins / total * 100

    return {
        'label': label,
        'threshold': threshold,
        'trades': total,
        'wins': wins,
        'wr': wr,
    }


def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ML íŒŒë™ íŒ¨í„´ ë¶„ì„                                               â•‘
â•‘   ë³¼ë¥¨/ë¸íƒ€ ì¶”ì„¸ í•™ìŠµ                                              â•‘
â•‘   Train: 2022-2023 | Test: 2024-2025                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # Load data
    print("Loading data...")
    htf_all = load_candles("BTCUSDT", "1h").to_pandas().set_index('time')
    ltf_all = load_candles("BTCUSDT", "15m").to_pandas().set_index('time')

    # ì—°ë„ë³„ ë¶„ë¦¬
    data_by_year = {}
    for year in [2022, 2023, 2024, 2025]:
        htf = htf_all[htf_all.index.year == year]
        ltf = ltf_all[ltf_all.index.year == year]
        if len(htf) > 100:
            data_by_year[year] = {'htf': htf, 'ltf': ltf}
            print(f"  {year}: HTF={len(htf)}, LTF={len(ltf)}")

    # Train/Test ë¶„ë¦¬
    htf_train = pd.concat([data_by_year[y]['htf'] for y in [2022, 2023] if y in data_by_year])
    ltf_train = pd.concat([data_by_year[y]['ltf'] for y in [2022, 2023] if y in data_by_year])
    htf_test = pd.concat([data_by_year[y]['htf'] for y in [2024, 2025] if y in data_by_year])
    ltf_test = pd.concat([data_by_year[y]['ltf'] for y in [2024, 2025] if y in data_by_year])

    print(f"\n  Train (2022-2023): HTF={len(htf_train)}, LTF={len(ltf_train)}")
    print(f"  Test (2024-2025): HTF={len(htf_test)}, LTF={len(ltf_test)}")

    # Build channels
    print("\nBuilding channels...")
    channels_train, _ = build_htf_channels(htf_train)
    channels_test, _ = build_htf_channels(htf_test)

    # Collect data
    print("\nCollecting training data...")
    train_df = collect_data(htf_train, ltf_train, channels_train)
    print(f"  Train samples: {len(train_df)} (Success: {train_df['success'].sum()}, {train_df['success'].mean()*100:.1f}%)")

    print("\nCollecting test data...")
    test_df = collect_data(htf_test, ltf_test, channels_test)
    print(f"  Test samples: {len(test_df)} (Success: {test_df['success'].sum()}, {test_df['success'].mean()*100:.1f}%)")

    # Train ML model
    print("\n" + "="*70)
    print("  ML ëª¨ë¸ í•™ìŠµ")
    print("="*70)

    model, scaler, train_pred, test_pred, train_proba, test_proba, importance = train_and_evaluate(train_df, test_df)

    # Feature importance
    print("\n  ğŸ“Š Feature Importance (Top 10)")
    for i, (feat, imp) in enumerate(list(importance.items())[:10]):
        print(f"    {i+1}. {feat:<25}: {imp:.4f}")

    # ê¸°ë³¸ ì„±ëŠ¥
    print("\n  ğŸ“ˆ ê¸°ë³¸ ì„±ëŠ¥ (threshold=0.5)")
    train_baseline_wr = train_df['success'].mean() * 100
    test_baseline_wr = test_df['success'].mean() * 100

    train_filtered_wr = train_df[train_pred == 1]['success'].mean() * 100 if (train_pred == 1).sum() > 0 else 0
    test_filtered_wr = test_df[test_pred == 1]['success'].mean() * 100 if (test_pred == 1).sum() > 0 else 0

    print(f"\n    Train:")
    print(f"      ê¸°ë³¸ WR: {train_baseline_wr:.1f}% ({len(train_df)}ê±´)")
    print(f"      ML í•„í„° WR: {train_filtered_wr:.1f}% ({(train_pred == 1).sum()}ê±´)")

    print(f"\n    Test:")
    print(f"      ê¸°ë³¸ WR: {test_baseline_wr:.1f}% ({len(test_df)}ê±´)")
    print(f"      ML í•„í„° WR: {test_filtered_wr:.1f}% ({(test_pred == 1).sum()}ê±´)")

    # Threshold ë³„ ì„±ëŠ¥
    print("\n" + "="*70)
    print("  Thresholdë³„ ì„±ëŠ¥ (Test)")
    print("="*70)
    print(f"\n  {'Threshold':<12} | {'ê±´ìˆ˜':>6} | {'WR':>8} | {'vs ê¸°ë³¸':>10}")
    print("-"*50)

    for threshold in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:
        result = backtest_with_filter(test_df, test_proba, threshold, f"p>={threshold}")
        if result and result['trades'] >= 10:
            diff = result['wr'] - test_baseline_wr
            print(f"  {threshold:<12} | {result['trades']:>6} | {result['wr']:>7.1f}% | {diff:>+9.1f}%")

    # LONG/SHORT ë³„ ë¶„ì„
    print("\n" + "="*70)
    print("  ë°©í–¥ë³„ ì„±ëŠ¥ (Test)")
    print("="*70)

    for direction in ['LONG', 'SHORT']:
        dir_mask = test_df['direction'] == direction
        dir_df = test_df[dir_mask]
        dir_proba = test_proba[dir_mask]

        baseline = dir_df['success'].mean() * 100
        print(f"\n  [{direction}] ê¸°ë³¸: {len(dir_df)}ê±´, WR {baseline:.1f}%")

        for threshold in [0.5, 0.6, 0.7]:
            result = backtest_with_filter(dir_df, dir_proba, threshold, f"p>={threshold}")
            if result and result['trades'] >= 5:
                diff = result['wr'] - baseline
                print(f"    p>={threshold}: {result['trades']}ê±´, WR {result['wr']:.1f}% ({diff:+.1f}%)")

    # ì—°ë„ë³„ ì¼ê´€ì„±
    print("\n" + "="*70)
    print("  ì—°ë„ë³„ ì¼ê´€ì„± (Test ë°ì´í„° ë‚´)")
    print("="*70)

    # 2024 vs 2025 ë¶„ë¦¬
    for year in [2024, 2025]:
        if year not in data_by_year:
            continue

        htf_year = data_by_year[year]['htf']
        ltf_year = data_by_year[year]['ltf']
        channels_year, _ = build_htf_channels(htf_year)
        year_df = collect_data(htf_year, ltf_year, channels_year)

        if len(year_df) == 0:
            continue

        # ì˜ˆì¸¡
        feature_cols = [
            'vol_vs_range', 'delta_vs_range', 'vol_zscore',
            'vol_trend', 'delta_trend', 'vol_accel', 'delta_accel',
            'vol_vs_wave', 'delta_vs_wave',
            'delta_aligned', 'delta_trend_aligned',
            'vol_spike', 'vol_low', 'delta_reversal',
            'cvd_wave', 'cvd_range',
            'body_ratio', 'is_bullish', 'wick_ratio',
            'is_long',
        ]
        X_year = scaler.transform(year_df[feature_cols].values)
        year_proba = model.predict_proba(X_year)[:, 1]

        baseline = year_df['success'].mean() * 100
        print(f"\n  [{year}] ê¸°ë³¸: {len(year_df)}ê±´, WR {baseline:.1f}%")

        for threshold in [0.5, 0.6, 0.7]:
            result = backtest_with_filter(year_df, year_proba, threshold, f"p>={threshold}")
            if result and result['trades'] >= 5:
                diff = result['wr'] - baseline
                print(f"    p>={threshold}: {result['trades']}ê±´, WR {result['wr']:.1f}% ({diff:+.1f}%)")

    # Summary
    print("\n" + "="*70)
    print("  ğŸ’¡ ìš”ì•½")
    print("="*70)
    print("""
  í•µì‹¬ í”¼ì²˜:
  - ë³¼ë¥¨/ë¸íƒ€ ì¶”ì„¸ (íŒŒë™ ë°©í–¥)
  - ë ˆì¸ì§€ ëŒ€ë¹„ í„°ì¹˜ ìº”ë“¤ ë°˜ì‘
  - ë¸íƒ€ ë°©í–¥ ì •ë ¬

  ë‹¤ìŒ ë‹¨ê³„:
  - ìµœì  threshold ì„ íƒ
  - Paper tradingì— ì ìš©
""")


if __name__ == "__main__":
    main()
